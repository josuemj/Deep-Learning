{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51070d9",
   "metadata": {},
   "source": [
    "# Hoja de Trabajo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a6103",
   "metadata": {},
   "source": [
    "- Josue Marroquin 22397\n",
    "- Sebastian Huertas 22295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88d20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b782a19",
   "metadata": {},
   "source": [
    "## Ejercicio 1 - Experimentación Práctica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113b9bb",
   "metadata": {},
   "source": [
    "### Task 1 - Preparación del conjunto de datos\n",
    "Cargue el conjunto de datos de Iris utilizando bibliotecas como sklearn.datasets. Luego, divida el conjunto de datos\n",
    "en conjuntos de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb10bec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información del conjunto de datos:\n",
      "Forma del dataset: (150, 4)\n",
      "Número de características: 4\n",
      "Número de muestras: 150\n",
      "Clases: ['setosa' 'versicolor' 'virginica']\n",
      "Distribución de clases: [50 50 50]\n",
      "\n",
      "Primeras 5 filas del dataset:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target species  \n",
      "0       0  setosa  \n",
      "1       0  setosa  \n",
      "2       0  setosa  \n",
      "3       0  setosa  \n",
      "4       0  setosa  \n",
      "\n",
      "Conjunto de entrenamiento: 120 muestras\n",
      "Conjunto de validación: 30 muestras\n",
      "Distribución en entrenamiento: [40 40 40]\n",
      "Distribución en validación: [10 10 10]\n",
      "\n",
      "Datos preparados exitosamente para entrenamiento y validación\n",
      "Variables disponibles:\n",
      "- X_train, X_val: características originales\n",
      "- X_train_scaled, X_val_scaled: características estandarizadas\n",
      "- y_train, y_val: etiquetas\n",
      "- df: DataFrame completo con información descriptiva\n"
     ]
    }
   ],
   "source": [
    "# Cargar el conjunto de datos de Iris\n",
    "iris = load_iris()\n",
    "X = iris.data  # Características: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # Etiquetas: 0=setosa, 1=versicolor, 2=virginica\n",
    "\n",
    "# Crear DataFrame para mejor visualización\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "df['species'] = df['target'].map({0: target_names[0], 1: target_names[1], 2: target_names[2]})\n",
    "\n",
    "print(\"Información del conjunto de datos:\")\n",
    "print(f\"Forma del dataset: {X.shape}\")\n",
    "print(f\"Número de características: {X.shape[1]}\")\n",
    "print(f\"Número de muestras: {X.shape[0]}\")\n",
    "print(f\"Clases: {target_names}\")\n",
    "print(f\"Distribución de clases: {np.bincount(y)}\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(\"\\nPrimeras 5 filas del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y validación (80% - 20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Mantener la proporción de clases en ambos conjuntos\n",
    ")\n",
    "\n",
    "print(f\"\\nConjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
    "print(f\"Conjunto de validación: {X_val.shape[0]} muestras\")\n",
    "print(f\"Distribución en entrenamiento: {np.bincount(y_train)}\")\n",
    "print(f\"Distribución en validación: {np.bincount(y_val)}\")\n",
    "\n",
    "# Estandarizar las características (opcional pero recomendado para muchos algoritmos)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(\"\\nDatos preparados exitosamente para entrenamiento y validación\")\n",
    "print(\"Variables disponibles:\")\n",
    "print(\"- X_train, X_val: características originales\")\n",
    "print(\"- X_train_scaled, X_val_scaled: características estandarizadas\")\n",
    "print(\"- y_train, y_val: etiquetas\")\n",
    "print(\"- df: DataFrame completo con información descriptiva\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
